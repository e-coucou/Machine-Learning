{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCfOEgqYdOuZ"
      },
      "source": [
        "# Human-level control through deep reinforcement learning\n",
        "\n",
        "![Texte alternatif…](https://media.discordapp.net/attachments/505714807291379724/525388603375878145/Breakout-Atari-2600-Wallpaper.gif?width=400&height=248)\n",
        "\n",
        "*The theory ofreinforcementlearning provides a normative account , deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment.Touse reinforcementlearning successfully insituations approaching real-world complexity, however, agents are confronted with a difficulttask:they must derive efficientrepresentations ofthe environment from high-dimensional sensory inputs, and use these to generalize past experience tonewsituations.Remarkably, humans and other animalsseem to solve this problem through a harmonious combination ofreinforcementlearning andhierarchicalsensoryprocessing systems , the former evidenced by a wealth of neural data revealingnotableparallelsbetweenthe phasic signals emittedbydopaminergic neurons and temporal difference reinforcement learning algorithms .While reinforcementlearning agents have achievedsome successesina variety of domains ,their applicability has previously beenlimited to domainsinwhich usefulfeatures can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advancesin training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learnsuccessfulpoliciesdirectly fromhigh-dimensionalsensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional humangamestester across a set of 49 games,using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions,resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.*\n",
        "\n",
        "\n",
        "> **Les succès du renforcement se limitaient (Avant ~ 2012/2013) à la résolution de tâche avec des inputs travaillés et à des espaces dimensionnels réduit.**\n",
        "\n",
        "> **Méthode : deep Q-Network \"end to end reinforcment learning\". L'apprentissage est fait de bout en bout en renforcement.**\n",
        "\n",
        "> **Résultat : un agent ayant seulement accès aux pixels de l'image est capable de surpasser tout les algorithmes précédemment mit en place et d'atteindre un score relativement égal ou meilleur que des joueurs pros sur 49 Jeux d'Atari. Texte en gras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIzOQbQtdV5j"
      },
      "source": [
        "*We set out to create a single algorithm that would be able to develop a wide range of competencies on a varied range of challenging tasks—a central goal of general artificial intelligence that has eluded previous effort. To achieve this,we developed anovel agent, a deepQ-network (DQN), which is able to combine reinforcement learning with a class of artificial neural network known as deep neural networks. Notably, recent advances in deep neural networks, in which several layers of nodes are used to build up progressively more abstractrepresentations of the data, have made it possible for artificial neural networksto learn concepts such as object categories directly from raw sensory data. We use one particularly successful architecture, the deep convolutional network, which uses hierarchical layers of tiled convolutional filters to mimic the effects ofreceptive fields—inspired byHubel andWiesel’s seminalwork on feedforward processing inearly visual cortex —thereby exploiting the localspatial correlations present in images, and building in robustness*\n",
        "\n",
        "> **Le but : obtenir un agent capable d'apprendre sur un énorme set d'environnement. Pour cela, ils ont décider de recourir au réseau de neuronne grâce à leur capacité de créer une représentation abstraire des données d'entrées**\n",
        "\n",
        "> ** Architecture : Deep Convolutional, neural network..**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvfJRfHuej4q"
      },
      "source": [
        "![Texte alternatif…](https://media.springernature.com/full/nature-static/assets/v1/image-assets/nature14236-f1.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3t3CdEOfNyg"
      },
      "source": [
        "![Texte alternatif…](https://pbs.twimg.com/media/B_PrYU3U0AAtL1K.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5prCjrzfV7A"
      },
      "source": [
        "*We considertasksin which the agent interacts with an environment through a sequence of observations, actions and rewards.The goal of the agent isto select actions in a fashion that maximizes cumulative future reward. More formally,we use a deep convolutional neural network to approximate the optimal action-value function.*\n",
        "\n",
        "\n",
        "> **L'agent est exposé à des séquences d'observation et récompense. Le but de l'agent est de sélectionner les actions de manière à maximiser le nombre de futures récompenses obtenu. Le modèle réalisant l'approximation de la fonction optimal calculant l'espérance de chaque action est donné par un réseau de neurones.. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fll9X8gzhRyo"
      },
      "source": [
        "![Texte alternatif…](https://github.com/thibo73800/aihub/blob/master/images/optimal_action_value_function.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-MjqaGqhqbC"
      },
      "source": [
        "*Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as Q) function20 . This instability has several causes: the correlations present in the sequence of observations,the fact that small updates toQ may significantly change the policy and therefore change the data distribution, and the correlations between the action-values(Q) and the target value. We addressthese instabilitieswith a novel variant of Q-learning,which uses two key ideas. F*\n",
        "*First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlationsin the observation sequence and smoothing over changesin the data distribution.\n",
        "Second,we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.*\n",
        "\n",
        "**>L'apprentissage par renforcement est instable ou diverge lorsqu'on introduit une fonction non-linéaire est utilisé pour faire l'approximation de la Q fonction. **\n",
        "\n",
        "**>L'instabilité à deux causes, la corrélation présente dans la séquence observé. **\n",
        "\n",
        "**>  Le fait qu'on l'on soit constamment en train de modifier Q, ce qui change la \"policy\" (les décisions que l'on prend) et ainsi change la distribution des données continuellement.**\n",
        "\n",
        "**> Et enfin, la corrélation direct entre l'approximation que l'onf fait de la Q function et la target. (voir formule).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPXH1ABsiExd"
      },
      "source": [
        "![Texte alternatif…](https://github.com/thibo73800/aihub/blob/master/images/q_function_loss_without_target.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9SInIuGim3B"
      },
      "source": [
        "### Solutions:\n",
        "\n",
        "> **Expérience replay : supprime la corrélation dans les données et réalise un changement moins brutal dans la distribution des données..**\n",
        "\n",
        "> **Target network: Utilise une copie de notre fonction Q pour calculer la target, et modifier cette copie seulement périodiquement pour ne pas sans cesse changer la target dans la formule d'erreur.**\n",
        "\n",
        "![Texte alternatif…](https://github.com/thibo73800/aihub/blob/master/images/q_learning_loss_with_target.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dv1X9NRhT9H"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v691XeP6oeZ"
      },
      "source": [
        "#Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iGP84fKg66uT"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from gym.wrappers import Monitor\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "import gym\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "wG5mbNpj9E2Q",
        "outputId": "65f392db-8edf-49c1-ef76-da1d65fbae10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 00:04:54.113821: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-05-24 00:04:54.113928: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/device:GPU:0']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1\n",
            "\n",
            "systemMemory: 16.00 GB\n",
            "maxCacheSize: 5.33 GB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "VALID_ACTIONS = [0, 1, 2, 3]\n",
        "\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "get_available_gpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw8k5PP57ZOB"
      },
      "source": [
        "## Préprocessing:\n",
        "\n",
        "*Preprocessing. Working directlywith rawAtari 2600 frames,which are 2103 160 pixel images with a 128-colour palette, can be demanding in terms of computation and memory requirements.We apply a basic preprocessing step aimed atreducing the input dimensionality and dealing with some artefacts of the Atari 2600 emulator. First,to encode a single framewe take themaximum value for each pixel colour value overthe frame being encoded and the previousframe. This was necessary to remove flickering that is present in games where some objects appear only in even frames while other objects appear only in odd frames, an artefact caused by the limited number of sprites Atari 2600 can display at once. Second, we then extract the Y channel, also known as luminance, from the RGB frame and rescale it to 84 3 84.The function w from algorithm 1 described belowappliesthis preprocessing to the m most recent frames and stacks them to produce the input to the Q-function, inwhichm 5 4, although the algorithm isrobust to different values of m (for example, 3 or 5).*\n",
        "\n",
        "> **Image en noir et blanc**\n",
        "\n",
        "> ** Image réduit à 84*84 **\n",
        "\n",
        "> ** On stack les 4 dernière images perçu. On à donc une image de (84, 84, 4) **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hr_DXIZY7cYh"
      },
      "outputs": [],
      "source": [
        "class StateProcessor():\n",
        "\n",
        "    def __init__(self):\n",
        "      with tf.compat.v1.variable_scope(\"process\"):\n",
        "        self.input_state = tf.compat.v1.placeholder(shape=[210, 160, 3], dtype=tf.uint8, name=\"input_process\")\n",
        "        self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
        "        self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
        "        self.output = tf.compat.v1.image.resize_images(    \n",
        "        self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "        self.output = tf.squeeze(self.output)\n",
        "\n",
        "    def process(self, sess, state):\n",
        "        return sess.run(self.output, { self.input_state: state })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv-LX6olCTYe"
      },
      "source": [
        "### Test the environement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "T5gXl7E0CWh3"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import time\n",
        "\n",
        "env = gym.make('Breakout-v0')\n",
        "env.reset()\n",
        "\n",
        "for _ in range(1000):\n",
        "    observation, reward, done, info = env.step(env.action_space.sample())\n",
        "    if done:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cY6KcfXCgvC"
      },
      "source": [
        "## Display and process the observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "yxvyqA_KCgPL",
        "outputId": "07e99096-3019-40b0-ab4d-a36c232eaca2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 00:05:14.345626: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-05-24 00:05:14.345669: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "2022-05-24 00:05:14.349089: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "dark"
          },
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 00:05:14.490288: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "dark"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  \n",
        "  plt.title(\"Original observation\")\n",
        "  plt.imshow(observation)\n",
        "  plt.show()\n",
        "\n",
        "  state_processor = StateProcessor()\n",
        "  process_obs = state_processor.process(sess, observation)\n",
        "  \n",
        "  plt.title(\"Process observation\")\n",
        "  plt.imshow(process_obs)\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXdOgdDP74nP"
      },
      "source": [
        "# DQN: Deep Q Network\n",
        "\n",
        "\n",
        "*There are several possible ways of parameterizing Q using a neural network. Because Q maps history–action pairs to scalar estimates of their Q-value, the history and the action have been used as inputsto the neural network by some previous approaches 24,26 . The main drawback of this type of architecture is that a separate forward pass is required to compute the Q-value of each action, resulting in a cost thatscaleslinearlywith the number of actions.We instead use an architecture in which there is a separate output unit for each possible action, and only the state representation is an input to the neural network. The outputs correspond to the predicted Q-values of the individual actions forthe inputstate. The main advantage of thistype of architecture isthe ability to compute Q-valuesfor all possible actionsin a given statewith only a single forward passthrough the network. The exact architecture, shown schematically in Fig. 1, is as follows. The input to the neural network consists of an 843 843 4 image produced by the preprocessing map w. The first hidden layer convolves 32 filters of 8 3 8 with stride 4with the input image and applies a rectifier nonlinearity31,32 . The second hidden layer convolves 64 filters of 4 3 4 with stride 2, again followed by a rectifier nonlinearity. Thisisfollowed by a third convolutional layerthat convolves 64 filters of 3 3 3with stride 1 followed by a rectifier. The final hidden layer is fully-connected and consists of 512 rectifier units. The output layer is a fully-connected linear layer with a single output for each valid action. The number of valid actions varied between 4 and 18 on the games we considered. *\n",
        "\n",
        "> **Input (84, 84, 4) **\n",
        "\n",
        "> **Convolution 32 filtres de 8*8 et des strides de 4 avec relu activation **\n",
        "\n",
        "> **Convolution 64 filtres de 4*4 avec des strides de 2 et une activation par relu**\n",
        "\n",
        "> **Convolution 64 filtres de 3*3 avec stride de 1 avec une activation par relu**\n",
        "\n",
        "> **Hidden layer: 542 avec activation par relu**\n",
        "\n",
        "> **Output layer: activation linéair avec autant d'unité que le nombre d'action disponible.**\n",
        "\n",
        "![Texte alternatif…](https://github.com/thibo73800/aihub/blob/master/images/q_learning_loss_with_target.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZcR_4CSp8CIe"
      },
      "outputs": [],
      "source": [
        "class DQN():\n",
        "\n",
        "    def __init__(self, scope):\n",
        "      self.scope = scope\n",
        "      with tf.compat.v1.variable_scope(self.scope):\n",
        "        self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # 4 Last frames of the game\n",
        "        self.X_pl = tf.compat.v1.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
        "        # The TD target value\n",
        "        self.y_pl = tf.compat.v1.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
        "        # Integer id of which action was selected\n",
        "        self.actions_pl = tf.compat.v1.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
        "\n",
        "        # Rescale the image\n",
        "        X = tf.compat.v1.to_float(self.X_pl) / 255.0\n",
        "        # Get the batch size\n",
        "        batch_size = tf.shape(self.X_pl)[0]\n",
        "\n",
        "        # Three convolutional layers\n",
        "        conv1 = tf.compat.v1.layers.conv2d(X, 32, 8, 4, activation=tf.nn.relu)\n",
        "        conv2 = tf.compat.v1.layers.conv2d(conv1, 64, 4, 2, activation=tf.nn.relu)\n",
        "        conv3 = tf.compat.v1.layers.conv2d(conv2, 64, 3, 1, activation=tf.nn.relu)\n",
        "\n",
        "        # Fully connected layers\n",
        "        flattened = tf.compat.v1.layers.flatten(conv3)\n",
        "        fc1 = tf.compat.v1.layers.dense(flattened, 512, activation=tf.nn.relu)\n",
        "        self.predictions = tf.compat.v1.layers.dense(fc1, len(VALID_ACTIONS))\n",
        "        tf.identity(self.predictions, name=\"predictions\")\n",
        "\n",
        "        # Get the predictions for the chosen actions only\n",
        "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
        "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
        "\n",
        "        # Calculate the loss\n",
        "        self.losses = tf.compat.v1.squared_difference(self.y_pl, self.action_predictions)\n",
        "        self.loss = tf.reduce_mean(self.losses)\n",
        "\n",
        "        # Optimizer Parameters from original paper\n",
        "        self.optimizer = tf.compat.v1.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
        "        self.train_op = self.optimizer.minimize(self.loss)\n",
        "\n",
        "    def predict(self, sess, s):\n",
        "        return sess.run(self.predictions, { self.X_pl: s })\n",
        "\n",
        "    def update(self, sess, s, a, y):\n",
        "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
        "        ops = [self.train_op, self.loss]\n",
        "        _, loss = sess.run(ops, feed_dict)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuDZqZSX-iNJ"
      },
      "source": [
        "### Copy Target Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LF_E07pj-nVY"
      },
      "outputs": [],
      "source": [
        "def copy_model_parameters(sess, estimator1, estimator2):\n",
        "    e1_params = [t for t in tf.compat.v1.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
        "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
        "    e2_params = [t for t in tf.compat.v1.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
        "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
        "\n",
        "    update_ops = []\n",
        "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
        "        op = e2_v.assign(e1_v)\n",
        "        update_ops.append(op)\n",
        "\n",
        "    sess.run(update_ops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtYske8X_yUs"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Hz7Lqitx_w-K",
        "outputId": "cd119f4e-2681-4529-ab70-ebae764e9213"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vw/2g_98rts7tzczmvh5hqt_qn00000gn/T/ipykernel_33554/794254304.py:22: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv1 = tf.compat.v1.layers.conv2d(X, 32, 8, 4, activation=tf.nn.relu)\n",
            "/Users/rky/miniforge3/envs/env-tf/lib/python3.9/site-packages/keras/legacy_tf_layers/convolutional.py:563: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/var/folders/vw/2g_98rts7tzczmvh5hqt_qn00000gn/T/ipykernel_33554/794254304.py:23: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv2 = tf.compat.v1.layers.conv2d(conv1, 64, 4, 2, activation=tf.nn.relu)\n",
            "/var/folders/vw/2g_98rts7tzczmvh5hqt_qn00000gn/T/ipykernel_33554/794254304.py:24: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  conv3 = tf.compat.v1.layers.conv2d(conv2, 64, 3, 1, activation=tf.nn.relu)\n",
            "/var/folders/vw/2g_98rts7tzczmvh5hqt_qn00000gn/T/ipykernel_33554/794254304.py:27: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
            "  flattened = tf.compat.v1.layers.flatten(conv3)\n",
            "/Users/rky/miniforge3/envs/env-tf/lib/python3.9/site-packages/keras/legacy_tf_layers/core.py:523: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/var/folders/vw/2g_98rts7tzczmvh5hqt_qn00000gn/T/ipykernel_33554/794254304.py:28: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  fc1 = tf.compat.v1.layers.dense(flattened, 512, activation=tf.nn.relu)\n",
            "/Users/rky/miniforge3/envs/env-tf/lib/python3.9/site-packages/keras/legacy_tf_layers/core.py:255: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n",
            "/var/folders/vw/2g_98rts7tzczmvh5hqt_qn00000gn/T/ipykernel_33554/794254304.py:29: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  self.predictions = tf.compat.v1.layers.dense(fc1, len(VALID_ACTIONS))\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# DQN\n",
        "dqn = DQN(scope=\"dqn\")\n",
        "# DQN target\n",
        "target_dqn = DQN(scope=\"target_dqn\")\n",
        "\n",
        "\n",
        "# State processor\n",
        "state_processor = StateProcessor()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvfBMnc9ENYI"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "0mUr61qPD0dc"
      },
      "outputs": [],
      "source": [
        "num_episodes = 10000\n",
        "\n",
        "replay_memory_size = 250000\n",
        "replay_memory_init_size = 50000\n",
        "\n",
        "update_target_estimator_every = 10000\n",
        "\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.1\n",
        "\n",
        "\n",
        "epsilon_decay_steps = 500000\n",
        "discount_factor = 0.99\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acIxMR-O_RWf"
      },
      "source": [
        "### Epsilon greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Jgj7Y-ey_Usm"
      },
      "outputs": [],
      "source": [
        "def make_epsilon_greedy_policy(estimator, nA):\n",
        "    def policy_fn(sess, observation, epsilon):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
        "        best_action = np.argmax(q_values)\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nj3TVr079M1x"
      },
      "outputs": [],
      "source": [
        "#saver = tf.train.Saver()\n",
        "start_i_episode = 0\n",
        "opti_step = -1\n",
        "\n",
        "# The replay memory\n",
        "replay_memory = []\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeBM7oh-Eprn"
      },
      "source": [
        "# Algorithm\n",
        "\n",
        "![Texte alternatif…](https://github.com/thibo73800/aihub/blob/master/images/algorithm_q_learning.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Chjh8mEUE0DS",
        "outputId": "fc9eac2f-65f2-4439-bab8-f48ab9212872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epsilon (1.0) ReplayMemorySize : (230) rSum: (nan) best_epi_reward: (0) OptiStep (-1) @ Episode 1/10000, loss: None"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 00:15:50.320244: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2022-05-24 00:15:50.320270: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
            "2022-05-24 00:15:50.329324: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
            "2022-05-24 00:15:50.489962: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
            "/Users/rky/miniforge3/envs/env-tf/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/Users/rky/miniforge3/envs/env-tf/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "2022-05-24 00:15:50.503286: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epsilon (1.0) ReplayMemorySize : (675) rSum: (5.0) best_epi_reward: (5.0) OptiStep (-1) @ Episode 2/10000, loss: None"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 00:15:53.351129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epsilon (1.0) ReplayMemorySize : (50301) rSum: (1.64) best_epi_reward: (5.0) OptiStep (-1) @ Episode 197/10000, loss: None00, loss: None"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-24 00:20:58.391576: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
            "2022-05-24 00:20:58.695387: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'trainable_variables'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000031?line=45'>46</a>\u001b[0m \u001b[39m# Update the target network\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000031?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m opti_step \u001b[39m%\u001b[39m update_target_estimator_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000031?line=47'>48</a>\u001b[0m     copy_model_parameters(sess, dqn, target_dqn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000031?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m Epsilon (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) ReplayMemorySize : (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) rSum: (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) best_epi_reward: (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) OptiStep (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) @ Episode \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, loss: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(epsilon, len_replay_memory, mean_epi_reward, best_epi_reward, opti_step, i_episode \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, num_episodes, loss), end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000031?line=50'>51</a>\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n",
            "\u001b[1;32m/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb Cell 23'\u001b[0m in \u001b[0;36mcopy_model_parameters\u001b[0;34m(sess, estimator1, estimator2)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000022?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy_model_parameters\u001b[39m(sess, estimator1, estimator2):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000022?line=1'>2</a>\u001b[0m     e1_params \u001b[39m=\u001b[39m [t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39;49mtrainable_variables() \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mstartswith(estimator1\u001b[39m.\u001b[39mscope)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000022?line=2'>3</a>\u001b[0m     e1_params \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(e1_params, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m v: v\u001b[39m.\u001b[39mname)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rky/Documents/GitHub/Machine-Learning/1-DQN/DQN.ipynb#ch0000022?line=3'>4</a>\u001b[0m     e2_params \u001b[39m=\u001b[39m [t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mtrainable_variables() \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mstartswith(estimator2\u001b[39m.\u001b[39mscope)]\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'trainable_variables'"
          ]
        }
      ],
      "source": [
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "  \n",
        "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "\n",
        "    # Used to save the model\n",
        "    checkpoint_dir = os.path.join(\"./\", \"checkpoints\")\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    saver = tf.compat.v1.train.Saver()\n",
        "    # Load a previous checkpoint if we find one\n",
        "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "    #  Epsilon decay\n",
        "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
        "    \n",
        "    # Policy\n",
        "    policy = make_epsilon_greedy_policy(dqn, len(VALID_ACTIONS))\n",
        "    \n",
        "    epi_reward = []\n",
        "    best_epi_reward = 0\n",
        "    \n",
        "    for i_episode in range(start_i_episode, num_episodes):      \n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "        state = state_processor.process(sess, state)\n",
        "        state = np.stack([state] * 4, axis=2)\n",
        "        loss = None\n",
        "        done = False\n",
        "        r_sum = 0\n",
        "        mean_epi_reward = np.mean(epi_reward)\n",
        "        if best_epi_reward < mean_epi_reward:\n",
        "            best_epi_reward = mean_epi_reward\n",
        "            saver.save(tf.compat.v1.get_default_session(), checkpoint_path)\n",
        "        \n",
        "        len_replay_memory = len(replay_memory)\n",
        "        while not done:\n",
        "            # Get the epsilon for this step\n",
        "            epsilon = epsilons[min(opti_step+1, epsilon_decay_steps-1)]\n",
        "        \n",
        "        \n",
        "            # Update the target network\n",
        "            if opti_step % update_target_estimator_every == 0:\n",
        "                copy_model_parameters(sess, dqn, target_dqn)\n",
        "            \n",
        "            print(\"\\r Epsilon ({}) ReplayMemorySize : ({}) rSum: ({}) best_epi_reward: ({}) OptiStep ({}) @ Episode {}/{}, loss: {}\".format(epsilon, len_replay_memory, mean_epi_reward, best_epi_reward, opti_step, i_episode + 1, num_episodes, loss), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "            \n",
        "            \n",
        "            #  Select an action with eps-greedy\n",
        "            action_probs = policy(sess, state, epsilon)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "\n",
        "            # Step in the env with this action\n",
        "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
        "            r_sum += reward\n",
        "            \n",
        "            # Add this action to the stack of images\n",
        "            next_state = state_processor.process(sess, next_state)\n",
        "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
        "            \n",
        "            # If our replay memory is full, pop the first element\n",
        "            if len(replay_memory) == replay_memory_size:\n",
        "                replay_memory.pop(0)\n",
        "            \n",
        "            \n",
        "            # Save transition to replay memory\n",
        "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
        "            \n",
        "            if len_replay_memory > replay_memory_init_size:\n",
        "                # Sample a minibatch from the replay memory\n",
        "                samples = random.sample(replay_memory, batch_size)                \n",
        "                states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
        "            \n",
        "                # We compute the next q value with                \n",
        "                q_values_next_target = target_dqn.predict(sess, next_states_batch)\n",
        "                t_best_actions = np.argmax(q_values_next_target, axis=1)\n",
        "                targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * q_values_next_target[np.arange(batch_size), t_best_actions]\n",
        "                \n",
        "                # Perform gradient descent update\n",
        "                states_batch = np.array(states_batch)\n",
        "                loss = dqn.update(sess, states_batch, action_batch, targets_batch)\n",
        "                \n",
        "                opti_step += 1\n",
        "                \n",
        "            state = next_state\n",
        "            if done:\n",
        "              break\n",
        "\n",
        "        epi_reward.append(r_sum)\n",
        "        if len(epi_reward) > 100:\n",
        "            epi_reward = epi_reward[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "LcOS-mRW8pt6",
        "outputId": "00b6a526-16e2-4dc5-b1e0-ac32f119091a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir(\"./checkpoints\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "AZafgTlKInZE",
        "outputId": "22d7765e-4a9a-45bc-ebe0-fecaed760784"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import time\n",
        "\n",
        "print(os.listdir(\"./checkpoints\"))\n",
        "\n",
        "for filename in os.listdir(\"./checkpoints\"):\n",
        "  print(\"Download\", filename)\n",
        "  files.download(os.path.join(\"./checkpoints\", filename))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('game_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "bc6c2409e22df4b6dc6c42eb7addfe820be784db604d44dacc946a75b91b0504"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
