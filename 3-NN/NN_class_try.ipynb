{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid():\n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def prime(y):\n",
    "        f = sigmoid.activation(y)\n",
    "        return (f * (1 - f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        # e_x = np.exp(x).reshape(-1,1)\n",
    "        e_x = np.exp(x - np.max(x,axis=-1).reshape(-1,1))\n",
    "        return e_x / e_x.sum(axis=-1).reshape(-1,1)\n",
    "    @staticmethod\n",
    "    def primeP(y): #marche pas\n",
    "        n = y.shape[1]\n",
    "        print(n,y.shape)\n",
    "        i = np.identity(n)\n",
    "        un = np.ones((n,y.shape[1]))\n",
    "        SM = y.reshape((-1,1))\n",
    "        print(SM.shape)\n",
    "        jac = np.diagflat(y) - np.dot(SM, SM.T)\n",
    "        j = np.dot((i* jac),un)\n",
    "        # x = np.dot(jac,y.T)\n",
    "        return j.T\n",
    "    @staticmethod\n",
    "    def prime(y):\n",
    "        return(y * (1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    '''\n",
    "    Performs the softmax activation on a given set of inputs\n",
    "    Input: x (N,k) ndarray (N: no. of samples, k: no. of nodes)\n",
    "    Returns: \n",
    "    Note: Works for 2D arrays only(rows for samples, columns for nodes/outputs)\n",
    "    '''\n",
    "    max_x = np.amax(x, 1).reshape(x.shape[0],1) # Get the row-wise maximum\n",
    "    e_x = np.exp(x - max_x ) # For stability\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax_grad(x): # Best implementation (VERY FAST)\n",
    "    '''Returns the jacobian of the Softmax function for the given set of inputs.\n",
    "    Inputs:\n",
    "    x: should be a 2d array where the rows correspond to the samples\n",
    "        and the columns correspond to the nodes.\n",
    "    Returns: jacobian\n",
    "    '''\n",
    "    s = Softmax(x)\n",
    "    a = np.eye(s.shape[-1])\n",
    "    temp1 = np.zeros((s.shape[0], s.shape[1], s.shape[1]),dtype=np.float32)\n",
    "    temp2 = np.zeros((s.shape[0], s.shape[1], s.shape[1]),dtype=np.float32)\n",
    "    temp1 = np.einsum('ij,jk->ijk',s,a)\n",
    "    temp2 = np.einsum('ij,ik->ijk',s,s)\n",
    "    return temp1-temp2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    def __init__(self, p, y,_m,**kwargs):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        self.m = _m\n",
    "        self.sw = kwargs.get('sample_weight',1)\n",
    "\n",
    "    def normalized(self,a,y):\n",
    "        self.p = a\n",
    "        self.y = y\n",
    "        self.c = self.y.shape[-1] #nb de class\n",
    "    def metrics(self):\n",
    "        r = (1 / self.m) * np.sum(self.forward()) /self.c\n",
    "        return r\n",
    "    def forward(self):\n",
    "    # (-y.log(p) - (1-y).log(1-p))\n",
    "        f = ((-self.y * np.log(self.p + EPSILON)) - ((1 - self.y) * (np.log(1 - self.p + EPSILON)))) * self.sw\n",
    "        return f\n",
    "    def backward(self):\n",
    "    # -y/p + (1-y)/(1-p)\n",
    "        # n = self.p.shape[0]\n",
    "        return (-self.y/(self.p + EPSILON) + (1 - self.y)/(1 - self.p + EPSILON)) * self.sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropy():\n",
    "    def __init__(self, a, y_true , _m, **kwargs):\n",
    "        self.p = a / (np.sum(a,axis=-1,keepdims=True)+EPSILON)\n",
    "        self.y = y_true\n",
    "        self.m = _m\n",
    "        self.sw = kwargs.get('sample_weight',1)\n",
    "    def normalized(self,a,y_true):\n",
    "        self.p = a / (np.sum(a,axis=-1,keepdims=True)+EPSILON)\n",
    "        self.y = y_true\n",
    "        # self.c = self.y.shape[-1] #nb de class\n",
    "    def metrics(self): # -y.log(p)\n",
    "        return (1 / self.m) * np.sum(self.forward())\n",
    "    def forward(self): # -y.log(p)\n",
    "        return (-self.y * np.log(self.p+EPSILON)) * self.sw\n",
    "    def backward(self):\n",
    "    # -y/p\n",
    "        return (-self.y/(self.p+EPSILON)) * self.sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE():\n",
    "    def __init__(self, p, y, _m, **kwargs):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        self.m = _m\n",
    "        self.sw = kwargs.get('sample_weight',1)\n",
    "    def normalized(self,a,y):\n",
    "        self.p = a\n",
    "        self.y = y\n",
    "        self.c = self.y.shape[-1] #nb de class\n",
    "    def forward(self):\n",
    "        return (np.square(self.p - self.y)) / self.c * self.sw\n",
    "    def backward(self):\n",
    "        return (2*(self.p - self.y)) * self.sw\n",
    "    def metrics(self):\n",
    "        return (1 / self.m) * (np.sum(self.forward()))\n",
    "        # return 1/self.m *np.sum(np.square(self.p - self.y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JEUX DE TEST \n",
    "* y : valeur TRUE\n",
    "* z : simulation valeur sortie couche z = wX + b\n",
    "* a : résultat couche activation SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32970771  4.52768052 -0.95453353  5.16596603  1.40572632 -0.1239037\n",
      "  -2.95130235  0.90576774 -0.41480254 -0.22734566]] [[4.99355964e-03 3.32327797e-01 1.38251979e-03 6.29173675e-01\n",
      "  1.46460052e-02 3.17255608e-03 1.87709251e-04 8.88361913e-03\n",
      "  2.37177533e-03 2.86078379e-03]] [1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(120)\n",
    "y = np.array([[0,0,0,0,1,0,0,0,0,0]])\n",
    "z = np.random.randn(1,10)/np.sqrt(2/10)\n",
    "a = softmax.activation(z)\n",
    "# print(a)\n",
    "print(z,a,a.sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.99355964e-03 3.32327797e-01 1.38251979e-03 6.29173675e-01\n",
      "  1.46460052e-02 3.17255608e-03 1.87709251e-04 8.88361913e-03\n",
      "  2.37177533e-03 2.86078379e-03]] [1.]\n"
     ]
    }
   ],
   "source": [
    "a2 = Softmax(z)\n",
    "print(a2,a2.sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5643484715378928\n",
      "[[ 4.99355964e-03  3.32327797e-01  1.38251979e-03  6.29173675e-01\n",
      "  -9.85353995e-01  3.17255608e-03  1.87709251e-04  8.88361913e-03\n",
      "   2.37177533e-03  2.86078379e-03]] (1, 10)\n",
      "[[ 4.99355964e-03  3.32327797e-01  1.38251979e-03  6.29173675e-01\n",
      "  -9.85353995e-01  3.17255608e-03  1.87709251e-04  8.88361913e-03\n",
      "   2.37177533e-03  2.86078379e-03]]\n"
     ]
    }
   ],
   "source": [
    "L = BinaryCrossEntropy(a,y,1)\n",
    "L.normalized(a,y)\n",
    "l = L.metrics()\n",
    "print(l)\n",
    "dL_da = L.backward()\n",
    "da_dz = softmax.prime(a)\n",
    "diff = dL_da*da_dz\n",
    "print(diff,y.shape)\n",
    "print(a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5222245   0.78520852  0.52002033  1.25547688 -5.68949143  0.52111153\n",
      "   0.51929353  0.52461186  0.52062303  0.52092127]]\n"
     ]
    }
   ],
   "source": [
    "da_dz_2 = Softmax_grad(a).reshape(10,10)\n",
    "# print(da_dz_2,dL_da)\n",
    "print(np.dot(da_dz_2,dL_da.T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (1, 10)\n",
      "(10, 1)\n",
      "[[-2.83507696e-01 -1.26607282e+01 -7.87769644e-02 -1.33128126e+01\n",
      "  -8.23455597e-01 -1.80450468e-01 -1.07086042e-02 -5.02392681e-01\n",
      "  -1.35011572e-01 -1.62768197e-01]]\n"
     ]
    }
   ],
   "source": [
    "da_dz_3 = softmax.primeP(a)\n",
    "# print(da_dz_3)\n",
    "print(np.dot(dL_da,da_dz_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep cce: 4.223587661845474\n",
      "tf cce: 4.2235876619127515\n"
     ]
    }
   ],
   "source": [
    "# Vérification CCE\n",
    "f_cce = CategoricalCrossEntropy(a,y,1)\n",
    "f_cce.normalized(a,y)\n",
    "e1 = f_cce.metrics()\n",
    "t_tf = tf.nn.softmax(z)\n",
    "t_cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "t1 = t_cce(y,t_tf).numpy()\n",
    "print('ep cce:', e1)\n",
    "print('tf cce:',t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4.99355964e-03 3.32327797e-01 1.38251979e-03 6.29173675e-01\n",
      "  1.46460052e-02 3.17255608e-03 1.87709251e-04 8.88361913e-03\n",
      "  2.37177533e-03 2.86078379e-03]], shape=(1, 10), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "t_tf = tf.nn.softmax(z)\n",
    "print(t_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep bce: 0.5643484715378928\n",
      "tf bce: 0.5643476765841015\n"
     ]
    }
   ],
   "source": [
    "t_bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "t = t_bce(y,t_tf).numpy()\n",
    "print('ep bce:', l)\n",
    "print('tf bce:', t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification metrics Binary Cross Entropy\n",
    "* EPSILON = 1e-12\n",
    "* utilisation sample_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24363858891514345\n",
      "ep: 0.24363858891514345\n",
      "tf: 0.24363858891670923\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[0, 1], [0, 0]])\n",
    "y_pred = np.array([[-18.6, 0.51], [2.94, -12.8]])\n",
    "sw = np.array([[0.8],[0.2]])\n",
    "ypS = sigmoid.activation(y_pred)\n",
    "# yp = np.clip(y_pred,1e-8,1e8)\n",
    "bce = BinaryCrossEntropy(ypS,y_true,2,sample_weight = sw)\n",
    "bce.normalized(ypS,y_true)\n",
    "bce_f = (bce.forward())\n",
    "print(np.mean(bce_f))\n",
    "l_bce = bce.metrics()\n",
    "print('ep:',l_bce)\n",
    "t_bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "print('tf:',t_bce(y_true,y_pred,sample_weight=sw).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification metrics Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf: 0.09524559936875954\n",
      "ep: 0.09524559936875655\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[0, 1], [0, 0]])\n",
    "y_pred = np.array([[-1.6, 0.51], [-1.94, -1.8]])\n",
    "sw = np.array([[0.8],[0.2]])\n",
    "ypS = sigmoid.activation(y_pred)\n",
    "t_ypS = tf.nn.sigmoid(y_pred)\n",
    "# print(t_ypS,ypS)\n",
    "t2_cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "t2 = t2_cce(y_true,t_ypS,sample_weight=sw).numpy()\n",
    "print('tf:',t2)\n",
    "e2_cce = CategoricalCrossEntropy(t_ypS,y_true,2,sample_weight=sw)\n",
    "e2_cce.normalized(t_ypS,y_true)\n",
    "e2 = e2_cce.metrics()\n",
    "print('ep:',e2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf: 0.051224278248531804\n",
      "ep: 0.051224278248531804\n"
     ]
    }
   ],
   "source": [
    "t_mse = tf.keras.losses.MeanSquaredError()\n",
    "tf_mse = t_mse(y_true,t_ypS).numpy()\n",
    "print('tf:',tf_mse)\n",
    "f_mse = MSE(t_ypS,y_true,2)\n",
    "f_mse.normalized(t_ypS,y_true)\n",
    "ep_mse = f_mse.metrics()\n",
    "print('ep:',ep_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60854981 0.39145019]\n",
      " [0.50219033 0.49780967]\n",
      " [0.181474   0.818526  ]\n",
      " [0.52390805 0.47609195]\n",
      " [0.66024491 0.33975509]\n",
      " [0.77328332 0.22671668]\n",
      " [0.90635004 0.09364996]\n",
      " [0.83287439 0.16712561]\n",
      " [0.61834897 0.38165103]\n",
      " [0.40778718 0.59221282]]\n"
     ]
    }
   ],
   "source": [
    "# x = (np.arange(0,10).reshape((10,1)) - 5 ) /10\n",
    "x = np.random.randn(10,2)\n",
    "# print(x)\n",
    "s = softmax()\n",
    "s_x = s.activation(x)\n",
    "print(s_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc6c2409e22df4b6dc6c42eb7addfe820be784db604d44dacc946a75b91b0504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
