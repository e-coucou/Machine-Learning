{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid():\n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        return y\n",
    "    @staticmethod\n",
    "    def prime(y):\n",
    "        f = sigmoid.activation(y)\n",
    "        return (f * (1 - f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    @staticmethod\n",
    "    def activation(x):\n",
    "        # e_x = np.exp(x).reshape(-1,1)\n",
    "        e_x = np.exp(x - np.max(x,axis=-1).reshape(-1,1))\n",
    "        return e_x / e_x.sum(axis=-1).reshape(-1,1)\n",
    "    @staticmethod\n",
    "    def primeP(y): #marche pas\n",
    "        n = y.shape[1]\n",
    "        print(n,y.shape)\n",
    "        i = np.identity(n)\n",
    "        un = np.ones((n,y.shape[1]))\n",
    "        SM = y.reshape((-1,1))\n",
    "        print(SM.shape)\n",
    "        jac = np.diagflat(y) - np.dot(SM, SM.T)\n",
    "        j = np.dot((i* jac),un)\n",
    "        # x = np.dot(jac,y.T)\n",
    "        return j.T\n",
    "    @staticmethod\n",
    "    def prime(y):\n",
    "        return(y * (1-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    '''\n",
    "    Performs the softmax activation on a given set of inputs\n",
    "    Input: x (N,k) ndarray (N: no. of samples, k: no. of nodes)\n",
    "    Returns: \n",
    "    Note: Works for 2D arrays only(rows for samples, columns for nodes/outputs)\n",
    "    '''\n",
    "    max_x = np.amax(x, 1).reshape(x.shape[0],1) # Get the row-wise maximum\n",
    "    e_x = np.exp(x - max_x ) # For stability\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax_grad(x): # Best implementation (VERY FAST)\n",
    "    '''Returns the jacobian of the Softmax function for the given set of inputs.\n",
    "    Inputs:\n",
    "    x: should be a 2d array where the rows correspond to the samples\n",
    "        and the columns correspond to the nodes.\n",
    "    Returns: jacobian\n",
    "    '''\n",
    "    s = Softmax(x)\n",
    "    a = np.eye(s.shape[-1])\n",
    "    temp1 = np.zeros((s.shape[0], s.shape[1], s.shape[1]),dtype=np.float32)\n",
    "    temp2 = np.zeros((s.shape[0], s.shape[1], s.shape[1]),dtype=np.float32)\n",
    "    temp1 = np.einsum('ij,jk->ijk',s,a)\n",
    "    temp2 = np.einsum('ij,ik->ijk',s,s)\n",
    "    return temp1-temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    def __init__(self, p, y,_m,**kwargs):\n",
    "        self.p = p\n",
    "        self.y = y\n",
    "        self.m = _m\n",
    "        self.sw = kwargs.get('sample_weight',1)\n",
    "\n",
    "    def normalized(self,a,y):\n",
    "        self.p = a\n",
    "        self.y = y\n",
    "        self.c = self.y.shape[-1] #nb de class\n",
    "    def metrics(self):\n",
    "        # r = (-1 / self.m) * (np.sum((self.y * np.log(self.p + 1e-8)) + ((1 - self.y) * (np.log(1 - self.p + 1e-8)))))/self.c\n",
    "        r = (1 / self.m) * np.sum(self.forward() * self.sw)\n",
    "        return r\n",
    "    def forward(self):\n",
    "    # (-y.log(p) - (1-y).log(1-p))\n",
    "        f = ((-self.y * np.log(self.p + 1e-8)) - ((1 - self.y) * (np.log(1 - self.p + 1e-8))))/self.c\n",
    "        return f\n",
    "    def backward(self):\n",
    "    # -y/p + (1-y)/(1-p)\n",
    "        # n = self.p.shape[0]\n",
    "        return (-self.y/(self.p+1e-8) + (1 - self.y)/(1 - self.p+1e-8)) * self.sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32970771  4.52768052 -0.95453353  5.16596603  1.40572632 -0.1239037\n",
      "  -2.95130235  0.90576774 -0.41480254 -0.22734566]] [[4.99355964e-03 3.32327797e-01 1.38251979e-03 6.29173675e-01\n",
      "  1.46460052e-02 3.17255608e-03 1.87709251e-04 8.88361913e-03\n",
      "  2.37177533e-03 2.86078379e-03]] [1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(120)\n",
    "y = np.array([[0,0,0,0,1,0,0,0,0,0]])\n",
    "z = np.random.randn(1,10)/np.sqrt(2/10)\n",
    "a = softmax.activation(z)\n",
    "# print(a)\n",
    "print(z,a,a.sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.99355964e-03 3.32327797e-01 1.38251979e-03 6.29173675e-01\n",
      "  1.46460052e-02 3.17255608e-03 1.87709251e-04 8.88361913e-03\n",
      "  2.37177533e-03 2.86078379e-03]] [1.]\n"
     ]
    }
   ],
   "source": [
    "a2 = Softmax(z)\n",
    "print(a2,a2.sum(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5643483920494577\n"
     ]
    }
   ],
   "source": [
    "L = BinaryCrossEntropy(a,y,1)\n",
    "L.normalized(a,y)\n",
    "l = L.metrics()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.99355959e-03  3.32327792e-01  1.38251978e-03  6.29173658e-01\n",
      "  -9.85353322e-01  3.17255605e-03  1.87709249e-04  8.88361904e-03\n",
      "   2.37177531e-03  2.86078376e-03]] (1, 10)\n",
      "[[ 4.99355964e-03  3.32327797e-01  1.38251979e-03  6.29173675e-01\n",
      "  -9.85353995e-01  3.17255608e-03  1.87709251e-04  8.88361913e-03\n",
      "   2.37177533e-03  2.86078379e-03]]\n"
     ]
    }
   ],
   "source": [
    "dL_da = L.backward()\n",
    "da_dz = softmax.prime(a)\n",
    "diff = dL_da*da_dz\n",
    "print(diff,y.shape)\n",
    "print(a-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52222413  0.785208    0.52001996  1.25547617 -5.68948762  0.52111116\n",
      "   0.51929316  0.52461149  0.52062266  0.5209209 ]]\n"
     ]
    }
   ],
   "source": [
    "da_dz_2 = Softmax_grad(a).reshape(10,10)\n",
    "# print(da_dz_2,dL_da)\n",
    "print(np.dot(da_dz_2,dL_da.T).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (1, 10)\n",
      "(10, 1)\n",
      "[[-2.83507465e-01 -1.26607178e+01 -7.87769002e-02 -1.33128018e+01\n",
      "  -8.23454927e-01 -1.80450321e-01 -1.07085955e-02 -5.02392272e-01\n",
      "  -1.35011462e-01 -1.62768065e-01]]\n"
     ]
    }
   ],
   "source": [
    "da_dz_3 = softmax.primeP(a)\n",
    "# print(da_dz_3)\n",
    "print(np.dot(dL_da,da_dz_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4.99355964e-03 3.32327797e-01 1.38251979e-03 6.29173675e-01\n",
      "  1.46460052e-02 3.17255608e-03 1.87709251e-04 8.88361913e-03\n",
      "  2.37177533e-03 2.86078379e-03]], shape=(1, 10), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "t_tf = tf.nn.softmax(z)\n",
    "print(t_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5643476765841015"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_tf = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "l_tf(y,t_tf).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8654578909902412\n",
      "[[8.35839003e-09 6.24806474e-01]\n",
      " [9.49788727e-01 2.76076495e-06]] 0.24363857325779462\n",
      "0.2436385925472071\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[0, 1], [0, 0]])\n",
    "y_pred = np.array([[-18.6, 0.51], [2.94, -12.8]])\n",
    "sw = np.array([[0.8],[0.2]])\n",
    "ypS = sigmoid.activation(y_pred)\n",
    "# yp = np.clip(y_pred,1e-8,1e8)\n",
    "bce = BinaryCrossEntropy(ypS,y_true,2,sample_weight = sw)\n",
    "bce.normalized(ypS,y_true)\n",
    "bce_f = (bce.forward())\n",
    "print(np.sum(bce_f)/2)\n",
    "l_bce = bce.metrics()\n",
    "print(ypS,l_bce)\n",
    "t_bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "print(t_bce(y_true,y_pred,sample_weight=[0.8, 0.2]).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc6c2409e22df4b6dc6c42eb7addfe820be784db604d44dacc946a75b91b0504"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
